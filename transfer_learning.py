# -*- coding: utf-8 -*-
"""transfer learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AYnAqVPqj4xvWgdzKom2STb1Zp9LrJzl

#transfer learning on ResNet

from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras.preprocessing import image   #for import images to algorithm line 2
from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions
import numpy as np

model = ResNet50(weights='imagenet')

image_path = 'elephant.jpg'
img = image.load_img(image_path, target_size=(224, 224))


x = image.img_to_array(img)   #we should convert image fron 'int' to 'image array'. this is because preparing 'img' to calculations (weights)
x = np.expand_dims(x, axis=0)   #we should add a dimention to everything we import. now we have 4D
x = preprocess_input(x)  #'preprocess_input' function for 'preprocessing'


preds = model.predict(x)


print('predicted:', decode_predictions(preds, top=3)[0])  #SoftMax layer

#print model summary

!pip install -q keras
import keras

#create base model
base_model = keras.applications.Xception(
    weights='imagenet',  # Load weights pre-trained on ImageNet.
    input_shape=(150, 150, 3),
    include_top=False) # Do not include the ImageNet classifier at the top.

#freexe base model
base_model.trainable = False

#create new model in top
inputs = keras.Input(shape=(150, 150, 3))
x = base_model(inputs, training=False)
x = keras.layers.GlobalAveragePooling2D()(x)
x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout
outputs = keras.layers.Dense(1)(x)
model = keras.Model(inputs, outputs)

loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)
optimizer = keras.optimizers.Adam()

'''model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])'''

print(model.summary())
#

!pip install -q keras
import keras

from tensorflow.keras.applications.vgg19 import VGG19
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.vgg19 import preprocess_input
import numpy as np

base_model = VGG19(weights='imagenet', include_top=False)
model = Model(inputs=base_model.input, outputs= base_model.get_layer('block5_conv4').output)



img_path = 'elephant.jpg'
img = image.load_img(img_path, target_size=(224, 224))

x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)
x = preprocess_input(x)

fc2 = Model.predict(x)

base_model = inceptionV3(weights='imagenet', include_top=False)

#add a global spatioal avarage pooling layer
x = base_model.output
x = GlobalAveragePooling2D()(x)

#let's add a fully connected layer
x = dence(1024, activation='relu')(x)

#and a logistic layer -- let's say we have 200 classes
predictions = Dense(200, activation='softmax')(x)

#this is the model we will train
model = Model(inputs=base_model.input, outputs=predictions)

#first: train only the top layers (which were randomly initialized)
#i.e. freez all convolutional inceptionV3 layers
for layer in base_model.layers:
  layer.trainable = false




#compile the model (should be done 'after' setting layers to non-trainable)
model.compile(optimizer='rmsprop'                     ''' rmsprop, adam, SGD, RMSprop, Adadelta, Adamax, Nadam''',
              loss='categorical_crossentropy'         ''' or binary_crossentropy (برای طبقه‌بندی دودویی), mean_squared_error(برای رگرسیون خطی.), categorical_crossentropy(برای طبقه‌بندی چند کلاسه)''',
              learning_rate=0.0001                    '''it is optional''',
              metrics= ['accuracy']                   '''or: mean_absolute_error(for regression), AUC,..'''
              )
#optimizer: By gradually updating the network weights, they seek to minimize the error and increase the accuracy of the model in predictions.
            #tasks: Adjust the weights frequently,
                    #The optimizer tries to steer the model towards a region of the parameter space where the error is minimized.
                    #Avoid getting stuck in 'local optimum points'





#train the model in the new data for a few epochs
model.fit(...)

#let's visualize layer name and layer indices to see how many layers
#we should freez:
for i, layer in enumerate(base_model.layers):
  print(i, layer.name)


#we chose to train the top 2 inception blocks, i.e. we will freeze
#the firs 249 layers and unfreeze the rest:
for layer in model.layers[:249]:
   layer.trainable = False
for layer in model.layers[249:]:
   layer.trainable = True


#we need to recompile the model for these modifications to take effect
#we use SGD with a low learning rate
from tensorflow.keras.optimizers import SGD
model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')


#we train our model again (this time fin-tuning the top 2 inception blocks
#alongside the top dense layers)
model.fit(...)

#this is the augmentation confirmation we will use for training
from tensorflow.keras.preprocessing.image import ImageDataGenerator
train_datagen = ImageDataGenerator(
    rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True)

#this is the augmentation confirmation we will use for testing:
#only rescaling
test_datagen = ImageDataGenerator(rescale=1./255)

# this is a generator that will read pictures found in
# subfolers of 'data/train', and indefinitely generate
# batches of augmented image

train_generator = train_datagen.flow_from_directory(
    'data/train',  # this is the target directory
    target_size=(150, 150),  # all images will be resized to 150x150
    batch_size=32,
    class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels


validation_generator = test_datagen.flow_from_directory(
    'data/validation',
    target_size=(150, 150),
    batch_size=32,
    class_mode='binary')


model.fit_generator(
    train_generator,
    steps_per_epoch=2000 // 32,
    epochs=50,
    validation_data=validation_generator,
    validation_steps=800 // 32)
#
"""

import tensorflow as tf

c = tf.constant([[3, 4, 3], [4, 5, 6], [5, 4, 7]])
c

c = tf.constant([[[3, 4, 3],[4, 6, 7]],[[3, 4, 3],[4, 6, 7]]])
c

r = tf.constant([3, 5])
f = tf.constant([7, 9])

rf = tf.add(r, f)
rf

fr = tf.subtract(r, f)
fr

m_1 = tf.constant([[4, 3, 5], [3, 5, 6]])
m_2 = tf.constant([[6, 6, 7], [2, 5, 7]])

mm = tf.multiply(m_1, m_2)
mm

#broadcasting

ty = tf.constant([1, 2, 3])
uy = tf.constant([4])     #عدد 4 را به صورت 4و 4و 4 در می آورد. به این میگویند برادکستینک

mmy = tf.multiply(ty, uy)
mmy

mat = tf.constant([4 , 5, 5, 7, 8, 1])

#reshap

reshap_mat = tf.reshape(mat, (2, 3))
reshap_mat

#expand dimention

mat = tf.constant([4 , 5, 5, 7, 8, 1])
expand_mat = tf.expand_dims(mat, axis=1)
expand_mat

mat = tf.constant([4 , 5, 5, 7, 8, 1])
min = tf.reduce_min(mat)    #Min
min

mat = tf.constant([[4 , 5, 5, 7, 8, 1], [5, 5, 4, 9, 0, 7]])

min = tf.reduce_min(mat, axis=0)     #با اکسیس میتوانیم تعیین کنیم که مینیمم را در سطرها حساب کند یا ستون ها
min

max = tf.constant([[4 , 5, 5, 7, 8, 1], [5, 5, 4, 9, 0, 7]])
max = tf.reduce_max(mat, axis=0)     #با اکسیس میتوانیم تعیین کنیم که مینیمم را در سطرها حساب کند یا ستون ها
max

avg = tf.reduce_mean(mat, axis=0)
avg

max = tf.constant([[4 , 5, 5, 7, 8, 1], [5, 5, 4, 9, 0, 7.0]])

std = tf.math.reduce_std(max)
std

max = tf.constant([[4 , 5, 5, 7, 8, 1], [5, 5, 4, 9, 0, 7.0]])
sum = tf.reduce_sum(max, axis=None)     #پس مفهوم اکسیس را درک کنید. معنی اش این است که در ابعاد ماتریس حرکت میکند.

import numpy as np
sum.numpy()

import tensorflow as tf
from tensorflow import keras

x = tf.constant([4, 6, 7, 8.0, 8, 2, 15, 34, 9])

y = 4 * 5 + x

y

#Modeling

import tensorflow as tf
from tensorflow import keras

model = keras.Sequential([
    keras.layers.Dense(100, activation='relu', input_shape=[1]),
    keras.layers.Dense(100),
    keras.layers.Dense(1)

])

model.compile(optimizer='sgd', loss= 'mean_squared_error')

model.fit(x, y, epochs=200, batch_size=1)

#test

x_test = tf.constant([5, 3, 7, 5, 6, 4, 23, 10, 3], dtype=float)
y_test = 4 * 5 + x_test

y_test

pred_y = model.predict(x_test)

pred_y

import matplotlib.pyplot as plt
import numpy as np

plt.plot(x_test, y_test, 'bo')
plt.plot(x_test, pred_y, 'ro')
plt.show()

import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib as plt

# y = x^2 + 5 * x + 1

x = np.arange(-80, 80, 0.5)
y = x ** 2 + 5 * x + 1

y.shape

#Data

from sklearn import model_selection
from sklearn.model_selection import train_test_split
import numpy as np

# y = x^2 + 5 * x + 1

x = np.arange(-80, 80, 0.5)
y = x ** 2 + 5 * x + 1

y.shape


x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

x_train.shape, x_test.shape

#Model 1

import tensorflow as tf
from tensorflow import keras

model = keras.Sequential()
model.add(keras.layers.Dense(1024, activation='relu', input_shape=[1]))
model.add(keras.layers.Dense(1))

model.compile(optimizer='adam', loss='mean_squared_error', )
hist_1 = model.fit(x_train, y_train, epochs=1200, batch_size=20, validation_data=(x_test, y_test))
#MSE=3000

#Model 2
import tensorflow as tf
from tensorflow import keras

model_2 = keras.Sequential()
model_2.add(keras.layers.Dense(256, activation='relu', input_shape=[1]))
model_2.add(keras.layers.Dense(256, activation='relu'))
model_2.add(keras.layers.Dense(256, activation='relu'))
model_2.add(keras.layers.Dense(256, activation='relu'))
model_2.add(keras.layers.Dense(1))

model_2.compile(optimizer='adam', loss='mean_squared_error')
#model_2.summary()
hist_2 = model_2.fit(x_train, y_train, batch_size= 64, epochs=1000, validation_data=(x_test, y_test))
#MSE = 362

y_pred_1 = model.predict(x_test)
y_pred_2 = model_2.predict(x_test)

import matplotlib.pyplot as plt

plt.scatter(x_test, y_test, color='red', linewidths= 0)
plt.scatter(x_test, y_pred_1, color='blue', linewidths= 0)
plt.scatter(x_test, y_pred_2, color= 'green', linewidths= 0)
plt.show()
#

plt.plot(hist_1.history['val_loss'][800:])
plt.plot(hist_2.history['val_loss'][800:], color= 'red')
plt.show()

# prompt: considering previous codes, I want to calculate computational cost of tow models

# Compute the number of parameters in each model.
num_params_1 = model.count_params()
num_params_2 = model_2.count_params()

# Compute the number of operations in each model.
# This is a rough estimate, as it does not take into account the specific hardware
# being used.
num_ops_1 = 2 * num_params_1  # Assume each operation is a matrix multiplication.
num_ops_2 = 4 * num_params_2  # Assume each operation is a matrix multiplication.

# Compute the computational cost of each model.
# This is a rough estimate, as it does not take into account the specific hardware
# being used.
computational_cost_1 = num_ops_1 * x_train.shape[0]
computational_cost_2 = num_ops_2 * x_train.shape[0]

# Print the computational cost of each model.
print("Computational cost of Model 1:", computational_cost_1)
print("Computational cost of Model 2:", computational_cost_2)

